{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run-cim.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Recognition Model (ASR) for Cook Islands MƒÅori\n",
        "Rolando Coto-Solano (rolando.a.coto.solano@dartmouth.edu)<br>\n",
        "Sally Akevai Nicholas (s.nicholas@massey.ac.nz)<br>\n",
        "Last modification: 20220111\n",
        "\n",
        "This code loads a language model trained using [XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2) and uses it to transcribe recordings in Cook Islands MƒÅori. It was trained using 4 hours of transcribed recordings from the [Paradisec CIM collection](https://catalog.paradisec.org.au/collections/SN1). The code is based on [Fine-tuning XLS-R for Multi-Lingual ASR with ü§ó Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) by [Patrick von Platen](https://huggingface.co/patrickvonplaten).\n",
        "\n",
        "The CIM model is licensed under the Kaitiakitanga license (https://github.com/TeHikuMedia/Kaitiakitanga-License), created by [Te Hiku Media](https://tehiku.nz/). You can use the model for non-profit purposes, but you must contact the authors to reuse it. Unless you are a member of the Cook Islands MƒÅori community, please do not attempt to the data from the model."
      ],
      "metadata": {
        "id": "aYu9YaXxXSrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation"
      ],
      "metadata": {
        "id": "EkcABL-hXgUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of XLSR"
      ],
      "metadata": {
        "id": "iPgipb2oXr-L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4hggC77hXOlB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers==4.4.0\n",
        "!pip install librosa\n",
        "!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html  > /dev/null\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "from transformers import Wav2Vec2Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model"
      ],
      "metadata": {
        "id": "83KvaxoiXwZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir cim-checkpoint-lrec\n",
        "!mkdir cim-checkpoint-lrec/checkpoint\n",
        "\n",
        "%cd cim-checkpoint-lrec\n",
        "!gdown --id 1SzP7ZhzZjkoq_LBdgHTBBtWd0dzMxd3d\n",
        "!tar -xf cim-asr-meta.tar.gz\n",
        "!rm cim-asr-meta.tar.gz\n",
        "\n",
        "%cd checkpoint\n",
        "!gdown --id 1O9FE1_KdgBgEBfgYWL-L2Bbqq1ZK5trd\n",
        "!tar -xf cim-asr-meta2.tar.gz\n",
        "!rm cim-asr-meta2.tar.gz\n",
        "!gdown --id 1-erDEc9uiSQsUUsq8KT3cHa4zNvGMejQ\n",
        "!gdown --id 1-xnsbUAQ8W9Gy8wpWpnL2KvfnyaBQvsL\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "xLixXO1_dJ06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download sample CIM files"
      ],
      "metadata": {
        "id": "KeY8YVugt8tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Akevai/CIM-ASR-Models/blob/main/sample-cim.wav?raw=true \n",
        "!mv sample-cim.wav?raw=true sample-cim.wav\n",
        "!wget https://raw.githubusercontent.com/Akevai/CIM-ASR-Models/main/sample-cim.csv"
      ],
      "metadata": {
        "id": "BDBe1KU-X3ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Audio Decoding"
      ],
      "metadata": {
        "id": "Rm4P7im1YP4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downsampling audio file"
      ],
      "metadata": {
        "id": "Mydq_wFYkLWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -i sample-cim.wav -ac 1 -ar 16000 temp-sample-cim.wav\n",
        "!rm sample-cim.wav\n",
        "!mv temp-sample-cim.wav sample-cim.wav"
      ],
      "metadata": {
        "id": "APU399Cdd4AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing XLSR-Wav2Vec2 and decoding the audio files"
      ],
      "metadata": {
        "id": "dBF_XUPTk5v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Load models\n",
        "pathCheckpoint = \"cim-checkpoint-lrec/checkpoint\"\n",
        "model = Wav2Vec2ForCTC.from_pretrained(pathCheckpoint).to(\"cuda\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"cim-checkpoint-lrec\")\n",
        "\n",
        "# Convert audio file to array\n",
        "def speech_file_to_array_fn(batch):\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = speech_array[0].numpy()\n",
        "    batch[\"sampling_rate\"] = sampling_rate\n",
        "    batch[\"target_text\"] = batch[\"sentence\"]\n",
        "    return batch\n",
        "\n",
        "# Prepare batch processing of files\n",
        "def prepare_dataset(batch):\n",
        "    # check that all files have the correct sampling rate\n",
        "    assert (\n",
        "        len(set(batch[\"sampling_rate\"])) == 1\n",
        "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
        "\n",
        "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "# Load CSV file with audio files to be transcribed\n",
        "dataTest = pd.read_csv(\"sample-cim.csv\") \n",
        "common_voice_test = Dataset.from_pandas(dataTest)\n",
        "\n",
        "# Extract features from audio files\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
        "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n",
        "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n",
        "\n",
        "# Process audio files\n",
        "input_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
        "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
        "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "# Decode audio files\n",
        "predictedText = processor.decode(pred_ids)\n",
        "\n",
        "# Replace the output transcription with a CIM orthographic output\n",
        "orthOrigin = ['ax', 'ex', 'ix', 'ox', 'ux', 'q']\n",
        "orthTarget = ['ƒÅ', 'ƒì', 'ƒ´', '≈ç', '≈´', 'Íûå']\n",
        "for i in range(0,len(orthOrigin)): predictedText = predictedText.replace(orthOrigin[i], orthTarget[i])"
      ],
      "metadata": {
        "id": "ptBxwzZiYcLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Check CIM transcription output"
      ],
      "metadata": {
        "id": "spa3kRqik03r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediction:\")\n",
        "print(predictedText)"
      ],
      "metadata": {
        "id": "uqjU-0V3t_32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play wave\n",
        "import IPython\n",
        "IPython.display.Audio('sample-cim.wav') # This is required on Google Colab due to compatibility issues"
      ],
      "metadata": {
        "id": "5-HaDjwpj1Wm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}